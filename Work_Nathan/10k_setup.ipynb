{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc9f19a7-d2d2-40ef-9768-e0a22bdd0e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "860c76d0-1316-4656-a74c-73ae79a13077",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\natha\\AppData\\Local\\Temp\\ipykernel_13896\\454242842.py:11: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "import re\n",
    "from wordfreq import top_n_list\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "import requests\n",
    "import requests_random_user_agent\n",
    "from tqdm.notebook import tqdm\n",
    "from nltk.stem import PorterStemmer\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import dask\n",
    "dask.config.set(scheduler=\"processes\")\n",
    "from tqdm.dask import TqdmCallback\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import glob\n",
    "from gensim.test.utils import datapath\n",
    "import logging\n",
    "import random\n",
    "from random import sample\n",
    "random.seed(1)\n",
    "from fpdf import FPDF\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "import pickle\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f535c176-8c8a-4af4-ae50-a1dc3e4e6875",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\natha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "update 14\n"
     ]
    }
   ],
   "source": [
    "%run useful_functions.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f7ddd9-b03a-4963-914f-16df23d285fb",
   "metadata": {},
   "source": [
    "## TOKENS part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe11b9c6-6608-4898-a3ae-825facc2357b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in the previously saved stocknames file\n",
    "stocknames = pd.read_csv(\"../../data/stocknames.csv.gz\", na_filter = False)\n",
    "stocknames.replace('', np.nan, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b425bd-da30-4eb5-9453-0d382990ae17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c277560a2df94313b121a1b9e67983e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing 10-Ks for 2021:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "request failed\n"
     ]
    }
   ],
   "source": [
    "#TEST #2017 2020 2021 2022 2023 2024\n",
    "#tokenize each sentence of each 10-K statements and save the resulting tockens\n",
    "save_path = '../../data/10k_statements_new/tokens/'\n",
    "years = np.arange(2022,2025)\n",
    "\n",
    "common_words = top_n_list('en', 100)\n",
    "stop_words = stopwords.words('english')\n",
    "alphabet = re.compile('[^a-z]')\n",
    "\n",
    "delayed_tokens = dask.delayed(get_tokens)\n",
    "\n",
    "list_missing_tokens=[]\n",
    "for year in [2021,2022,2023,2017]: #years \n",
    "    #take the urls of all the 10-K fillings for each year\n",
    "    urls = stocknames.filter(like = 'url_{}'.format(year))\n",
    "    urls.index = stocknames.ticker\n",
    "    urls = urls.dropna()\n",
    "    \n",
    "    #compute in batches\n",
    "    len_batch = 250\n",
    "    #ceil of euclidian division\n",
    "    nb_batches = -(len(urls) // -len_batch)\n",
    "    start = 0\n",
    "    end = len_batch\n",
    "    for batch in tqdm(range(nb_batches), desc = 'Tokenizing 10-Ks for {}'.format(year),leave = False):\n",
    "        for j in range(10): #j attempt to create allpromises list\n",
    "            #get a list of tokens for each sentence of each document\n",
    "            allpromises = []\n",
    "            for ticker, url in urls.iloc[start:end,:].itertuples():\n",
    "                text=None\n",
    "                for i in range(10): #i attempt to request text from url\n",
    "                    try:\n",
    "                        text = requests.get(url).text\n",
    "                    except:\n",
    "                        print(\"request failed\")\n",
    "                        time.sleep(120)\n",
    "    \n",
    "                    if text!=None:\n",
    "                        break\n",
    "                        \n",
    "                if text==None:\n",
    "                    print(\"after 10 attempts (20 min), the server still failed to respond, shutting down program...\")\n",
    "                    stop\n",
    "                            \n",
    "                        \n",
    "                allpromises.append(delayed_tokens(text, ticker = ticker))\n",
    "    \n",
    "               \n",
    "            there_was_no_error=True\n",
    "            with TqdmCallback(desc = 'Tokenizing 10-Ks in batch', leave = False):\n",
    "                \n",
    "    \n",
    "                try:\n",
    "                    tokens = dask.compute(allpromises)[0]\n",
    "    \n",
    "    \n",
    "                except Exception as e:\n",
    "                            # Generate a random 6-digit number\n",
    "                            random_number = random.randint(100000, 999999)\n",
    "\n",
    "                    \n",
    "                            print(\"error with tokens, error #\"+str(random_number))\n",
    "                            there_was_no_error=False\n",
    "                            list_missing_tokens.append(allpromises)\n",
    "    \n",
    "                                            \n",
    "                            #with open(str(year)+'missing_tokens_'+str(random_number)+'.pickle', 'wb') as f:\n",
    "                            #    pickle.dump(list_missing_tokens, f)\n",
    "    \n",
    "    \n",
    "            #save the tokens\n",
    "            if there_was_no_error:\n",
    "                for dict_ in tokens:\n",
    "                            ticker = list(dict_.keys())[0]\n",
    "                            tokens_df = pd.DataFrame(dict_[ticker])\n",
    "                            tokens_df.to_csv(save_path+'{}/{}_tokens.csv.gz'.format(year, ticker))\n",
    "\n",
    "                start += len_batch\n",
    "                end += len_batch\n",
    "                break\n",
    "\n",
    "            else:\n",
    "                print(\"rebuilding the 'allpromises' list, iteration : \"+str(j))\n",
    "                time.sleep(60)\n",
    "\n",
    "            if j==9:\n",
    "                print(\"impossible to build 'allpromises' after 10 iterations, shutting down program...\")\n",
    "                stop\n",
    "                        \n",
    "                \n",
    "\n",
    "    time.sleep(120)\n",
    "    \n",
    "clear_output()\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f900ba2b-f74c-4f9a-8ffb-06d2e351a84a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5491bb5c-9adf-4954-9a68-3ad203fa196d",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_path = '../../data/10k_statements_new/tokens/{}/*'\n",
    "#print summary statistics of the paragraphs from the 10-Ks\n",
    "stats = []\n",
    "index = []\n",
    "for year in years:\n",
    "    files = glob.glob(token_path.format(year))\n",
    "    allpromises = []\n",
    "    for file in files:\n",
    "        allpromises.append(get_token_stats(file))\n",
    "        ticker = re.findall('\\d/(.*)_tokens.csv.gz', file)[0]\n",
    "        index.extend([ticker+'_'+str(year)])\n",
    "        \n",
    "    with TqdmCallback(desc = 'Computing stats on tokens for {}'.format(year), leave = False):\n",
    "        stats.extend(dask.compute(allpromises)[0])\n",
    "        \n",
    "clear_output()\n",
    "pd.DataFrame(stats, columns = ['nb_paragraphs', 'avg_paragraph_len'], index = index).dropna().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3bd4b8b-d811-48cc-9915-6abd215025da",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(stats, columns = ['nb_paragraphs', 'avg_paragraph_len'], index = index).dropna().hist(bins = 50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9258dc02-441e-4dd0-95bd-3847bfdabc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from here, there is code I don't currently used but I still want to have save somewhere"
   ]
  },
  {
   "cell_type": "raw",
   "id": "41897e48-65a4-4fab-bd3b-db8defe3da35",
   "metadata": {},
   "source": [
    "#tokenize each sentence of each 10-K statements and save the resulting tockens\n",
    "save_path = '../../data/10k_statements_new/tokens/'\n",
    "years = np.arange(2022,2025)\n",
    "\n",
    "common_words = top_n_list('en', 100)\n",
    "stop_words = stopwords.words('english')\n",
    "alphabet = re.compile('[^a-z]')\n",
    "\n",
    "delayed_tokens = dask.delayed(get_tokens)\n",
    "\n",
    "list_missing_tokens=[]\n",
    "for year in [2019,2020,2021,2022,2023,2024]: #years #NOTE 2017 need revision\n",
    "    #take the urls of all the 10-K fillings for each year\n",
    "    urls = stocknames.filter(like = 'url_{}'.format(year))\n",
    "    urls.index = stocknames.ticker\n",
    "    urls = urls.dropna()\n",
    "    \n",
    "    #compute in batches\n",
    "    len_batch = 250\n",
    "    #ceil of euclidian division\n",
    "    nb_batches = -(len(urls) // -len_batch)\n",
    "    start = 0\n",
    "    end = len_batch\n",
    "    for batch in tqdm(range(nb_batches), desc = 'Tokenizing 10-Ks for {}'.format(year),\n",
    "                     leave = False):\n",
    "        #get a list of tokens for each sentence of each document\n",
    "        allpromises = []\n",
    "        for ticker, url in urls.iloc[start:end,:].itertuples():\n",
    "            text=None\n",
    "            for i in range(10):\n",
    "                try:\n",
    "                    text = requests.get(url).text\n",
    "                except:\n",
    "                    print(\"request failed\")\n",
    "                    time.sleep(120)\n",
    "\n",
    "                if text!=None:\n",
    "                    break\n",
    "                    \n",
    "            if text==None:\n",
    "                print(\"after 10 attempts (20 min), the server still failed to respond, shutting down program...\")\n",
    "                stop\n",
    "                        \n",
    "                    \n",
    "            allpromises.append(delayed_tokens(text, ticker = ticker))\n",
    "\n",
    "           \n",
    "        there_was_no_error=True\n",
    "        with TqdmCallback(desc = 'Tokenizing 10-Ks in batch', leave = False):\n",
    "            \n",
    "\n",
    "            try:\n",
    "                tokens = dask.compute(allpromises)[0]\n",
    "\n",
    "\n",
    "            except Exception as e:\n",
    "                        print(\"aie : \",year,\" , \",allpromises)\n",
    "                        print(\"################################################\")\n",
    "                        print(\"################################################\")\n",
    "                        print(\"################################################\")\n",
    "                        print(e)\n",
    "                        print(\"################################################\")\n",
    "                        print(\"################################################\")\n",
    "                        print(\"################################################\")\n",
    "\n",
    "                        there_was_no_error=False\n",
    "                        list_missing_tokens.append(allpromises)\n",
    "\n",
    "                        # Generate a random 6-digit number\n",
    "                        random_number = random.randint(100000, 999999)\n",
    "                        \n",
    "                        \n",
    "                                        \n",
    "                        with open(str(year)+'missing_tokens_'+str(random_number)+'.pickle', 'wb') as f:\n",
    "                            pickle.dump(list_missing_tokens, f)\n",
    "\n",
    "\n",
    "        #save the tokens\n",
    "        if there_was_no_error:\n",
    "            for dict_ in tokens:\n",
    "                        ticker = list(dict_.keys())[0]\n",
    "                        tokens_df = pd.DataFrame(dict_[ticker])\n",
    "                        tokens_df.to_csv(save_path+'{}/{}_tokens.csv.gz'.format(year, ticker))\n",
    "                    \n",
    "            \n",
    "\n",
    "        \n",
    "        \n",
    "        start += len_batch\n",
    "        end += len_batch\n",
    "    time.sleep(120)\n",
    "    \n",
    "clear_output()\n",
    "print('Done')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505637ba-325c-441e-928b-e7c41f7affbb",
   "metadata": {},
   "source": [
    "## item 1A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e0bd81-ff23-4817-adab-7489ad39be71",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = '../../data/10k_statements_new/item_1A_limits/'\n",
    "years = np.arange(2007,2025)\n",
    "\n",
    "delayed_tokens = dask.delayed(get_tokens)\n",
    "for year in years:\n",
    "    limits_df = pd.DataFrame()\n",
    "    #take the urls of all the 10-K fillings for each year\n",
    "    urls = stocknames.filter(like = 'url_{}'.format(year))\n",
    "    urls.index = stocknames.ticker\n",
    "    urls = urls.dropna()\n",
    "    \n",
    "    #compute in batches\n",
    "    len_batch = 250\n",
    "    #ceil of euclidian division\n",
    "    nb_batches = -(len(urls) // -len_batch)\n",
    "    start = 0\n",
    "    end = len_batch\n",
    "    for batch in tqdm(range(nb_batches), desc = 'Finding item 1A of 10-Ks for {}'.format(year),\n",
    "                     leave = False):\n",
    "        #get the index of the first and last paragraph of Item 1A of each document\n",
    "        allpromises = []\n",
    "        for ticker, url in urls.iloc[start:end,:].itertuples():\n",
    "            text = requests.get(url).text\n",
    "            allpromises.append(delayed_tokens(text, ticker = ticker, find_item1A_ = True))\n",
    "\n",
    "        with TqdmCallback(desc = 'Finding item 1A of 10-Ks in batch', leave = False):\n",
    "            limits = dask.compute(allpromises)[0]\n",
    "\n",
    "        #save the limits\n",
    "        temp = pd.concat([pd.DataFrame(l) for l in limits], axis=1)\n",
    "        limits_df = pd.concat([limits_df, temp], axis=1)\n",
    "        \n",
    "        start += len_batch\n",
    "        end += len_batch\n",
    "    \n",
    "    limits_df.index = ['start', 'stop']\n",
    "    limits_df.to_csv(save_path+'{}_limits.csv'.format(year))\n",
    "    time.sleep(90)\n",
    "    \n",
    "clear_output()\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f181955a-a040-4ab8-ab46-41291758040e",
   "metadata": {},
   "outputs": [],
   "source": [
    "limit_path = '../../data/10k_statements_new/item_1A_limits/{}_limits.csv'\n",
    "limits = pd.DataFrame()\n",
    "for year in years:\n",
    "    temp = pd.read_csv(limit_path.format(year), index_col = 0)\n",
    "    temp.index = temp.index+'_'+str(year)\n",
    "    limits = pd.concat([limits, temp], axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff057d28-2d1f-41c7-9156-b6ea1149a8c2",
   "metadata": {},
   "source": [
    "## Paragraph vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df69a88-a443-4cf5-96df-72976a325d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_path = '../../data/10k_statements_new/tokens/{}/*'\n",
    "save_path = '../../data/10k_statements_new/paragraph_vectors/'\n",
    "years = np.arange(2007,2025)\n",
    "model_ = 'model_dbow_v200_e50_w15'\n",
    "\n",
    "#Save the vectors for all firm-year observations\n",
    "for year in years:\n",
    "    files = glob.glob(token_path.format(year))\n",
    "    \n",
    "    model = get_best_model()\n",
    "\n",
    "    for i,file in enumerate(tqdm(files, desc = f'Vectorizing paragraphs for {year}', leave = False)):\n",
    "        ticker = re.findall('\\d/(.*)_tokens.csv.gz', file)[0]\n",
    "        tockens = pd.read_csv(file, index_col = 0, dtype = 'object').T\n",
    "\n",
    "        vects = []\n",
    "        sentences = []\n",
    "        for j, col in enumerate(tockens.columns):\n",
    "            v = get_vect(model, tockens[col].dropna().values, '10k{}_{}'.format(0,j))\n",
    "            vects.append(v)\n",
    "        \n",
    "        vects = pd.DataFrame(vects)\n",
    "        vects.to_csv(save_path+'{}/{}_vectors.csv.gz'.format(year, ticker), index = False)\n",
    "\n",
    "clear_output()\n",
    "print('Done')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
