{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1694db3-77fb-46d5-8ba9-6ac26d562714",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\natha\\Desktop\\17-Cyber-risk_and_the_cross-section_of_stock_returns-main\\17-Cyber-risk_and_the_cross-section_of_stock_returns-main\\Work_Nathan\\useful_functions.py:941: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  item1A = re.sub('\\s{2,}',' ', item1A)\n",
      "C:\\Users\\natha\\Desktop\\17-Cyber-risk_and_the_cross-section_of_stock_returns-main\\17-Cyber-risk_and_the_cross-section_of_stock_returns-main\\Work_Nathan\\useful_functions.py:952: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  document = re.sub('\\s{2,}',' ', document)\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\natha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "update 16\n"
     ]
    }
   ],
   "source": [
    "%run useful_functions.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "517b4615-4e6e-46c5-b6cc-8d16ee4e2603",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "import re\n",
    "from wordfreq import top_n_list\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "import requests\n",
    "import requests_random_user_agent\n",
    "from tqdm.notebook import tqdm\n",
    "from nltk.stem import PorterStemmer\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import dask\n",
    "dask.config.set(scheduler=\"processes\")\n",
    "from tqdm.dask import TqdmCallback\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import glob\n",
    "from gensim.test.utils import datapath\n",
    "import logging\n",
    "import random\n",
    "from random import sample\n",
    "random.seed(1)\n",
    "from fpdf import FPDF\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "import pickle\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ee1083c-8649-41a4-9e13-2ded049c3376",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in the previously saved stocknames file\n",
    "stocknames = pd.read_csv(\"../../data/stocknames.csv.gz\", na_filter = False)\n",
    "stocknames.replace('', np.nan, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31629ce1-23ff-4760-b2e7-f4c1eeb33846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "save_path = '../../data/10k_statements_new/financial_text_variables/'\n",
    "years = np.arange(2020,2025)\n",
    "\n",
    "common_words = top_n_list('en', 100)\n",
    "stop_words = stopwords.words('english')\n",
    "alphabet = re.compile('[^a-z]')\n",
    "\n",
    "delayed_text = dask.delayed(get_cleaned_text)\n",
    "\n",
    "list_missing_tokens=[]\n",
    "for year in years: #years \n",
    "    #take the urls of all the 10-K fillings for each year\n",
    "    urls = stocknames.filter(like = 'url_{}'.format(year))\n",
    "    urls.index = stocknames.ticker\n",
    "    urls = urls.dropna()\n",
    "    \n",
    "    #compute in batches\n",
    "    len_batch = 250\n",
    "    #ceil of euclidian division\n",
    "    nb_batches = -(len(urls) // -len_batch)\n",
    "    start = 0\n",
    "    end = len_batch\n",
    "    for batch in tqdm(range(nb_batches), desc = 'Cleaning text 10-Ks for {}'.format(year),leave = False):\n",
    "        for j in range(10): #j attempt to create allpromises list\n",
    "            #get a list of tokens for each sentence of each document\n",
    "            allpromises = []\n",
    "            for ticker, url in urls.iloc[start:end,:].itertuples():\n",
    "                text=None\n",
    "                for i in range(10): #i attempt to request text from url\n",
    "                    try:\n",
    "                        text = requests.get(url).text\n",
    "                    except:\n",
    "                        print(\"request failed\")\n",
    "                        time.sleep(120)\n",
    "    \n",
    "                    if text!=None:\n",
    "                        break\n",
    "                        \n",
    "                if text==None:\n",
    "                    print(\"after 10 attempts (20 min), the server still failed to respond, shutting down program...\")\n",
    "                    stop\n",
    "                            \n",
    "                        \n",
    "                allpromises.append(delayed_text(text, ticker = ticker, find_item1A_=True))\n",
    "    \n",
    "               \n",
    "            there_was_no_error=True\n",
    "            with TqdmCallback(desc = 'Cleaning text 10-Ks in batch', leave = False):\n",
    "                \n",
    "                #cleaned_text = dask.compute(allpromises)[0]\n",
    "                try:\n",
    "                    cleaned_text = dask.compute(allpromises)[0]\n",
    "    \n",
    "    \n",
    "                except Exception as e:\n",
    "                            # Generate a random 6-digit number\n",
    "                            random_number = random.randint(100000, 999999)\n",
    "\n",
    "                    \n",
    "                            print(\"error with tokens, error #\"+str(random_number))\n",
    "                            there_was_no_error=False\n",
    "                            list_missing_tokens.append(allpromises)\n",
    "    \n",
    "                            \n",
    "                            #with open(str(year)+'missing_tokens_'+str(random_number)+'.pickle', 'wb') as f:\n",
    "                            #    pickle.dump(list_missing_tokens, f)\n",
    "    \n",
    "    \n",
    "            #save the tokens\n",
    "            if there_was_no_error:\n",
    "                for tuple in cleaned_text:\n",
    "\n",
    "                    \n",
    "                            ticker_name = tuple[0]\n",
    "                            list_text = tuple[1]\n",
    "                            first_indice_1A=tuple[2]\n",
    "                            last_indice_1A=tuple[3]\n",
    "                    \n",
    "                            readability=tuple[4]\n",
    "                            secret=secrets(list_text)\n",
    "                            risk_len=risk_section_length(first_indice_1A,last_indice_1A)\n",
    "\n",
    "                            np.save(save_path+str(year)+\"/\"+ticker_name+'_readability_secret_risklength.npy', np.array([readability,secret,risk_len]))\n",
    "\n",
    "                         \n",
    "                            \n",
    "\n",
    "                start += len_batch\n",
    "                end += len_batch\n",
    "                break\n",
    "\n",
    "            else:\n",
    "                print(\"rebuilding the 'allpromises' list, iteration : \"+str(j))\n",
    "                time.sleep(60)\n",
    "\n",
    "            if j==9:\n",
    "                print(\"impossible to build 'allpromises' after 10 iterations, shutting down program...\")\n",
    "                stop\n",
    "                        \n",
    "                \n",
    "\n",
    "    time.sleep(120)\n",
    "    \n",
    "clear_output()\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb31faa3-c63d-4221-9ba9-c0305b384b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "#file:///C:/Users/natha/Desktop/Litterature_Review/Sentiment%20Analysis/sautnerLentVilkovZhang2023.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
